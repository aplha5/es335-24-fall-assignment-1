{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1-d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the `Task2_1a`, `Task2_1b`, `Task2_1c` you will get the outputs as \n",
    "\n",
    "```task2_1a\n",
    "Task_1a\n",
    "Accuracy: 0.6111\n",
    "Precision: 0.5966\n",
    "Recall: 0.6111\n",
    "Confusion Matrix:\n",
    "[[3 2 1 3 0 0]\n",
    " [2 5 1 1 0 0]\n",
    " [5 2 1 1 0 0]\n",
    " [0 0 0 8 1 0]\n",
    " [0 1 0 1 7 0]\n",
    " [0 0 0 0 0 9]]\n",
    "\n",
    "Task2_1b\n",
    "Accuracy: 0.8889\n",
    "Precision: 0.8998\n",
    "Recall: 0.8889\n",
    "Confusion Matrix:\n",
    "[[6 2 1 0 0 0]\n",
    " [1 8 0 0 0 0]\n",
    " [0 2 7 0 0 0]\n",
    " [0 0 0 9 0 0]\n",
    " [0 0 0 0 9 0]\n",
    " [0 0 0 0 0 9]]\n",
    "\n",
    "\n",
    "Task2_1c\n",
    "Accuracy: 0.9327\n",
    "Precision: 0.9328\n",
    "Recall: 0.9327\n",
    "Confusion Matrix:\n",
    "[[486  18  13   0   0   0]\n",
    " [ 22 416  24   0   1   0]\n",
    " [  9  20 393   0   0   0]\n",
    " [  0   0   0 489  44   0]\n",
    " [  0   0   0  57 515   0]\n",
    " [  0   0   0   0   0 583]]\n",
    "\n",
    "From the above outputs you can observe that the 3rd model is giving best accuracy, precision and recall and less miss classifications in confusion matrix than the other so by this we can conclude that the 3rd  model is the best model among the three i.e, the model created with the given featured\n",
    "dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify participants or activities where the model performance is poor, you can analyze the confusion matrix for each model. The confusion matrix provides insights into how well the model predicts each class (activity) and where it misclassifies them.\n",
    "```h\n",
    "The confusion matrix rows and columns are arranged as \n",
    "1-WALKING,\n",
    "2-WALKING_UPSTAIRS,\n",
    "3-WALKING_DOWNSTAIRS,\n",
    "4-SITTING,\n",
    "5-STANDING,\n",
    "6-LAYING\n",
    "By seeing the confusion matrices we can say that:\n",
    "In model-1:\n",
    "  - if we observe row 1 and column one we can see that among 9 true instances only 3 were correctly classified - walking\n",
    "  - In row 3 column 3 we can see that among 9 true instances only 1 instance is correctly classified - walking-downstairs\n",
    "In model-2:\n",
    "  - model 2 is better than model 1 as there are only very less misclassifications.\n",
    "  - in walking only 3 were misclassified and in walking-downstairs only 2 were missclassified.\n",
    "In model-3:\n",
    "  - Model 3 is far more better than all the models as we can see comparitively less missclassifications were there \n",
    "\n",
    "By looking at all the results, the activities Walking, Walking-upstairs, Walking-downstairs are mostly misclassified.\n",
    "The misclassification can be because of:\n",
    " - class imbalance : If the dataset has fewer instances of certain activities (e.g., WALKING_DOWNSTAIRS) compared to others, the model might not have learned those activities as effectively, leading to a higher rate of misclassification.\n",
    " - Noise in Data Collection: Background noise or external vibrations during data collection could introduce noise into the accelerometer data, leading to potential misclassification.\n",
    " - Feature Limitations: The features extracted from the accelerometer data might not fully capture the nuances of certain activities. For example, features like mean acceleration or variance might not be sufficient to differentiate between WALKING and WALKING_UPSTAIRS, leading to potential confusion.\n",
    " - Underfitting or Overfitting : If the data is more bias or the data has high variance also the missclassification can happen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```k\n",
    "If we run the Task4_1 i.e, where the model is trained with the uci-har raw accelerometer dataset and tested against the activites data  collected by us, we get the output as \n",
    "New Data Accuracy: 0.2500\n",
    "New Data Precision: 0.3651\n",
    "New Data Recall: 0.2500\n",
    "New Data Confusion Matrix:\n",
    "[[1 0 0 1 0 2]\n",
    " [0 0 0 0 0 4]\n",
    " [0 0 1 0 0 3]\n",
    " [0 0 0 0 0 4]\n",
    " [0 0 0 0 0 4]\n",
    " [0 0 0 0 0 4]]\n",
    "\n",
    " By seeing the metrics we can say that  it appears that the model's performance is significantly lower compared to when it was trained and tested on the original UCI-HAR dataset.\n",
    " - The accuracy of 0.25 (25%) is quite low, indicating that the model correctly classified only 25% of the new data. This suggests that the model is struggling to generalize to the new data you collected.\n",
    " - Precision of 0.3651 (36.51%) indicates that when the model predicted a certain activity, it was correct only 36.51% of the time. This suggests that the model is making many false-positive predictions, where it predicts an activity that is not actually present.\n",
    " - Recall of 0.25 (25%) implies that the model is only able to correctly identify 25% of the actual instances of each activity. This means it's missing a significant number of true positives, failing to recognize the correct activity most of the time.\n",
    " - By seeing confusion matrix only Laying was completly correctly classified remaning all are amost incoorectly classified.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```p\n",
    "If we run the Task4_2 i.e, where the model is trained with the collected activities dataset and tested against the same data  collected by doing 70-30 train-test split, we get the output as \n",
    "Accuracy: 0.2500\n",
    "Precision: 0.1667\n",
    "Recall: 0.3333\n",
    "Confusion Matrix:\n",
    "[[0 0 0 0 0 1]\n",
    " [1 0 0 1 0 0]\n",
    " [0 0 0 2 0 0]\n",
    " [0 0 0 0 1 0]\n",
    " [0 0 0 0 1 0]\n",
    " [0 0 0 0 0 1]]\n",
    "\n",
    " By seeing at the metrics we can that the model has even performed poor than the above model and only standing and laying were correctly classified.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few-shot prompting is a technique used in machine learning, especially in natural language processing, where a model is given a small number of examples to learn from. This is particularly useful when there's limited data available for training.\n",
    "```p\n",
    "By running the Task4_3.py i.e, using UCI-HAR dataset as training dataset and collected activities data as test data, we are getting the below metrics.\n",
    "Accuracy: 0.2083\n",
    "Precision: 0.1852\n",
    "Recall: 0.2083\n",
    "Confusion Matrix:\n",
    "[[2 0 1 0 0 1]\n",
    " [4 0 0 0 0 0]\n",
    " [3 0 1 0 0 0]\n",
    " [4 0 0 0 0 0]\n",
    " [3 0 0 0 0 1]\n",
    " [2 0 0 0 0 2]]\n",
    "\n",
    " By observing accuracy, precision , recall we can say that the model using few-shot promt is performing slightly lower than the decision tree model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running Task4_4.py i.e, by using the collected activities data for training and testing by splitting it as 70-30 I got the metrics as\n",
    "```p\n",
    "Accuracy: 0.3750\n",
    "Precision: 0.2917\n",
    "Recall: 0.4167\n",
    "Confusion Matrix:\n",
    "[[0 1 0 0 0 0]\n",
    " [1 1 0 0 0 0]\n",
    " [0 2 0 0 0 0]\n",
    " [0 0 0 1 0 0]\n",
    " [0 0 0 1 0 0]\n",
    " [0 0 0 0 0 1]]\n",
    " By looking at the metrics we can say that the model using few-shot promting is performing more bteer than the normal decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 stages of the model \n",
    " - Underfitting - high bias(accuracy increasing phase)\n",
    " - Bestfitting - balanced bias and variance(constant accuracy is maintained)\n",
    " - Overfitting - high variance(accuracy decreasing phase)\n",
    "\n",
    "The optimum depth is achived at the end of the underfitting and statrt of the best fit in the given case the tree has its optimum depth as 7.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree implemented by us and the desicion tree impoted from the scikit-learn almost performed similarly in terms of RMSE and MAE\n",
    "```p\n",
    "Custom Decision Tree - RMSE: 3.304607642786025, MAE: 2.2856647893142683\n",
    "Scikit-learn Decision Tree - RMSE: 3.3088512768357115, MAE: 2.310981245010471 \n",
    "\n",
    "By looking at the metrics we can say that the custom desicion tree which was implemented by us is performing very slight better than the scikit-learn desicion tree as the root mean sqaue error(rsme) of cutsom decision tree is slightly lower than that for scikit-learn decision tree\n",
    "and also mae is also less for the custom one which means that model has smaller prediction errors on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
